Multiple Linear Regression using Gradient Descent
Overview
This project implements Multiple Linear Regression using Gradient Descent in pure Python (without external libraries like NumPy or Scikit-learn). The goal is to train a model that predicts an output variable based on multiple input features.

1. Understanding Multiple Linear Regression
Multiple Linear Regression models the relationship between a dependent variable 
ğ‘¦
y and multiple independent variables 
ğ‘‹
1
,
ğ‘‹
2
,
â€¦
,
ğ‘‹
ğ‘›
X 
1
â€‹
 ,X 
2
â€‹
 ,â€¦,X 
n
â€‹
  using the following equation:

ğ‘¦
=
ğ‘
0
+
ğ‘
1
ğ‘‹
1
+
ğ‘
2
ğ‘‹
2
+
â‹¯
+
ğ‘
ğ‘›
ğ‘‹
ğ‘›
y=b 
0
â€‹
 +b 
1
â€‹
 X 
1
â€‹
 +b 
2
â€‹
 X 
2
â€‹
 +â‹¯+b 
n
â€‹
 X 
n
â€‹
 
where:

ğ‘
0
b 
0
â€‹
  is the intercept (bias term).
ğ‘
1
,
ğ‘
2
,
â€¦
,
ğ‘
ğ‘›
b 
1
â€‹
 ,b 
2
â€‹
 ,â€¦,b 
n
â€‹
  are the coefficients (weights) for each independent variable.
The goal is to determine the best values for 
ğ‘
0
,
ğ‘
1
,
â€¦
,
ğ‘
ğ‘›
b 
0
â€‹
 ,b 
1
â€‹
 ,â€¦,b 
n
â€‹
  that minimize the prediction error.

2. Cost Function (Mean Squared Error)
To measure how well the model fits the data, we use the Mean Squared Error (MSE):

ğ‘€
ğ‘†
ğ¸
=
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
(
ğ‘¦
ğ‘–
âˆ’
ğ‘¦
^
ğ‘–
)
2
MSE= 
n
1
â€‹
  
i=1
âˆ‘
n
â€‹
 (y 
i
â€‹
 âˆ’ 
y
^
â€‹
  
i
â€‹
 ) 
2
 
where:

ğ‘¦
ğ‘–
y 
i
â€‹
  is the actual value.
ğ‘¦
^
ğ‘–
=
ğ‘
0
+
ğ‘
1
ğ‘‹
ğ‘–
1
+
ğ‘
2
ğ‘‹
ğ‘–
2
+
â‹¯
+
ğ‘
ğ‘›
ğ‘‹
ğ‘–
ğ‘›
y
^
â€‹
  
i
â€‹
 =b 
0
â€‹
 +b 
1
â€‹
 X 
i1
â€‹
 +b 
2
â€‹
 X 
i2
â€‹
 +â‹¯+b 
n
â€‹
 X 
in
â€‹
  is the predicted value.
ğ‘›
n is the total number of data points.
Our goal is to minimize this error.

3. Gradient Descent Optimization
Since MSE depends on 
ğ‘
0
,
ğ‘
1
,
â€¦
,
ğ‘
ğ‘›
b 
0
â€‹
 ,b 
1
â€‹
 ,â€¦,b 
n
â€‹
 , we use Gradient Descent to update them iteratively.

Computing Gradients
To minimize MSE, we compute the partial derivatives of the cost function with respect to each coefficient:

âˆ‚
ğ‘€
ğ‘†
ğ¸
âˆ‚
ğ‘
ğ‘—
=
âˆ’
2
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘‹
ğ‘–
ğ‘—
(
ğ‘¦
ğ‘–
âˆ’
ğ‘¦
^
ğ‘–
)
âˆ‚b 
j
â€‹
 
âˆ‚MSE
â€‹
 =âˆ’ 
n
2
â€‹
  
i=1
âˆ‘
n
â€‹
 X 
ij
â€‹
 (y 
i
â€‹
 âˆ’ 
y
^
â€‹
  
i
â€‹
 )
for each 
ğ‘—
=
0
,
1
,
2
,
â€¦
,
ğ‘›
j=0,1,2,â€¦,n.

When 
ğ‘—
=
0
j=0, 
ğ‘‹
ğ‘–
ğ‘—
=
1
X 
ij
â€‹
 =1 (for the bias term).
These gradients tell us how to adjust the coefficients to reduce error.
Updating Coefficients
Using the gradients, we update the coefficients iteratively:

ğ‘
ğ‘—
=
ğ‘
ğ‘—
âˆ’
ğ›¼
âˆ‚
ğ‘€
ğ‘†
ğ¸
âˆ‚
ğ‘
ğ‘—
b 
j
â€‹
 =b 
j
â€‹
 âˆ’Î± 
âˆ‚b 
j
â€‹
 
âˆ‚MSE
â€‹
 
where:

ğ›¼
Î± is the learning rate (step size for updates).
This process is repeated until the model converges (or for a fixed number of iterations).

ğŸ“Œ Summary
âœ… Multiple Linear Regression extends simple linear regression to multiple features.
âœ… We minimize Mean Squared Error (MSE) to find the best coefficients.
âœ… Gradient Descent iteratively updates the coefficients until convergence.
âœ… The learning rate 
ğ›¼
Î± determines how fast the model learns.

ğŸš€ Happy Coding!

